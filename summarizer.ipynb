{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "import requests\n",
    "from requests.exceptions import HTTPError\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import json\n",
    "import time\n",
    "from urllib.parse import quote\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from rouge_score import rouge_scorer\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "from bert_score import score as bert_score\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from fuzzywuzzy import fuzz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EntityFinder: Extracts named entities from text and sorts them based on frequency.\n",
    "\n",
    "Inputs: raw text or a file path\n",
    "\n",
    "Outputs: a sorted list of named entities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityFinder:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"en_core_web_trf\")\n",
    "        \n",
    "    def ner(self, input_text):\n",
    "        if os.path.isfile(input_text):\n",
    "            with open(input_text, \"r\", encoding=\"utf-8\") as file:\n",
    "                text = file.read()\n",
    "        else:\n",
    "            text = input_text\n",
    "\n",
    "        doc = self.nlp(text)\n",
    "\n",
    "        entities = [(ent.text, ent.label_) for ent in doc.ents if ent.label_ in [\"PERSON\", \"ORG\", \"GPE\", \"EVENT\", \"WORK_OF_ART\",\"DATE\",\"NORP\",\"LAW\"]]\n",
    "        return entities\n",
    "\n",
    "    def get_sorted_entities(self, entities):\n",
    "        \n",
    "        entity_counter = Counter(entities)\n",
    "        sorted_entities = [entity for entity, count in entity_counter.most_common()]\n",
    "        \n",
    "        return sorted_entities\n",
    " \n",
    "# entity_finder = EntityFinder()\n",
    "# extracted_entities = entity_finder.ner(\"InputText.txt\")\n",
    "# sorted_entities = entity_finder.get_sorted_entities(extracted_entities)\n",
    "# for entity in sorted_entities:\n",
    "#     print(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CacheManager: manages a singleton cache for storing and retrieving Wikidata entities, ensuring that cached data persists across program runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CacheManager:\n",
    "    _instance = None  # \n",
    "\n",
    "    def __new__(cls, use_cache=True):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super(CacheManager, cls).__new__(cls)\n",
    "            cls._instance.use_cache = use_cache\n",
    "            cls._instance.cache_file = \"wikidata_cache.json\"\n",
    "            cls._instance.wikidata_cache = cls._instance.load_cache() if use_cache else {}\n",
    "        return cls._instance  \n",
    "\n",
    "    def load_cache(self):\n",
    "        try:\n",
    "            with open(self.cache_file, \"r\") as f:\n",
    "                return json.load(f)\n",
    "        except (FileNotFoundError, json.JSONDecodeError):\n",
    "            with open(self.cache_file, \"w\") as f:\n",
    "                json.dump({}, f)\n",
    "            return {}\n",
    "    \n",
    "    def save_cache(self): \n",
    "        if self.use_cache:\n",
    "            with open(self.cache_file, \"w\") as f:\n",
    "                json.dump(self.wikidata_cache, f, indent=4)\n",
    "\n",
    "    def get(self, entity_name):\n",
    "        return self.wikidata_cache.get(entity_name)\n",
    "\n",
    "    def update(self, entity_name, data):\n",
    "        self.wikidata_cache[entity_name] = data\n",
    "        self.save_cache()\n",
    "\n",
    "# cache_manager = CacheManager(use_cache=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EntityLinker: links named entities to Wikidata by retrieving their IDs and labels, using a caching system to minimize redundant API requests.\n",
    "\n",
    "Inputs: a list of entity tuples ((entity_name, entity_type)).\n",
    "\n",
    "Outputs: a dictionary containing entity names and their Wikidata information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityLinker:\n",
    "    def __init__(self):\n",
    "        self.cache_manager = CacheManager()  \n",
    "    \n",
    "    def link_entities(self, sorted_entities):\n",
    "        wikidata_entities = {}\n",
    "\n",
    "        for entity_name, entity_type in sorted_entities:\n",
    "            cached_data = self.cache_manager.get(entity_name)\n",
    "            if self.cache_manager.use_cache and cached_data:\n",
    "                print(f\"Retrieved {entity_name} from cache\")\n",
    "                wikidata_entities[entity_name] = {\n",
    "                    \"info\": cached_data[\"info\"],\n",
    "                    \"type\": cached_data.get(\"type\", entity_type)\n",
    "                }\n",
    "            else:\n",
    "                print(f\"Fetching {entity_name} from Wikidata API\")\n",
    "                url = f'https://www.wikidata.org/w/api.php?action=wbsearchentities&search={entity_name}&language=en&format=json'\n",
    "                response = requests.get(url)\n",
    "                data = response.json()\n",
    "\n",
    "                if 'search' in data and data['search']:\n",
    "                    entity_id = data['search'][0]['id']\n",
    "                    entity_label = data['search'][0]['label']\n",
    "\n",
    "                    entity_info = {\n",
    "                        \"info\": (entity_id, entity_label),\n",
    "                        \"triples\": [],\n",
    "                        \"type\": entity_type\n",
    "                    }\n",
    "\n",
    "                    # Cache the entity\n",
    "                    if self.cache_manager.use_cache:\n",
    "                        self.cache_manager.update(entity_name, entity_info)\n",
    "\n",
    "                    wikidata_entities[entity_name] = entity_info\n",
    "\n",
    "        return wikidata_entities\n",
    "    \n",
    "\n",
    "# entity_linker = EntityLinker()\n",
    "# wikidata_entities = entity_linker.link_entities(sorted_entities)\n",
    "# print(wikidata_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KnowledgeExtractor: extracts factual knowledge from Wikidata by querying entity relationships, using caching to reduce redundant API calls.\n",
    "\n",
    "Inputs: a dictionary of Wikidata entities with their IDs and types.\n",
    "\n",
    "Outputs: a list of formatted knowledge statements extracted from Wikidata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeExtractor:\n",
    "    def __init__(self):\n",
    "        self.cache_manager = CacheManager() \n",
    "        self.query_templates = self.open_queries()\n",
    "        self.sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "        self.sparql.setReturnFormat(JSON)\n",
    "\n",
    "    def open_queries(self):\n",
    "        \"\"\"Load the query templates from the JSON file.\"\"\"\n",
    "        with open(\"wikidata_queries.json\", \"r\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def get_triples(self, entity_id, entity_type):\n",
    "        query = self.query_templates[entity_type].replace(\"{entity_id}\", entity_id)\n",
    "        self.sparql.setQuery(query)\n",
    "        results = self.sparql.query().convert()\n",
    "\n",
    "        triples = [\n",
    "            (\n",
    "                result.get('predicateLabel', {}).get('value', 'Unknown Relationship'),\n",
    "                result.get('objectLabel', {}).get('value', 'Unknown Object')\n",
    "            )\n",
    "            for result in results['results']['bindings']\n",
    "        ]\n",
    "\n",
    "        return triples if triples else []\n",
    "\n",
    "    def extract_knowledge(self, wikidata_entities):\n",
    "        extracted_knowledge = []\n",
    "\n",
    "        for entity_name, entity_data in wikidata_entities.items():\n",
    "            entity_id = entity_data[\"info\"][0]\n",
    "            entity_label = entity_data[\"info\"][1]\n",
    "            entity_type = entity_data[\"type\"]\n",
    "            \n",
    "            triples = []\n",
    "            \n",
    "            if self.cache_manager.use_cache:\n",
    "                cached_data = self.cache_manager.get(entity_name)\n",
    "                if cached_data and len(cached_data[\"triples\"]) > 0:\n",
    "                    triples = cached_data[\"triples\"]\n",
    "                    print(f\"Using cached triples for {entity_name}\")\n",
    "                else:\n",
    "                    print(f\"Fetching triples for {entity_name} from Wikidata...\")\n",
    "                    triples = self.get_triples(entity_id, entity_type)\n",
    "                    \n",
    "                    \n",
    "                    if cached_data:\n",
    "                        cached_data[\"triples\"] = triples\n",
    "                        self.cache_manager.update(entity_name, cached_data)\n",
    "                    else:\n",
    "                        \n",
    "                        self.cache_manager.update(entity_name, {\"triples\": triples})\n",
    "            else:\n",
    "                print(f\"Fetching triples for {entity_name} from Wikidata (cache disabled)...\")\n",
    "                triples = self.get_triples(entity_id, entity_type)\n",
    "\n",
    "            for triple in triples:\n",
    "                knowledge = f\"{entity_label} - {' - '.join(triple)}\"\n",
    "                extracted_knowledge.append(knowledge)\n",
    "\n",
    "        return extracted_knowledge\n",
    "    \n",
    "\n",
    "# knowledge_extractor = KnowledgeExtractor()  \n",
    "# extracted_knowledge = knowledge_extractor.extract_knowledge(wikidata_entities)\n",
    "# print (extracted_knowledge)\n",
    "# # for knowledge in extracted_knowledge:\n",
    "# #     print(knowledge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KnowledgeOptimizer: reduces redundancy and organizes Wikidata triples into a structured format by grouping relationships under their respective subjects.\n",
    "\n",
    "Inputs: a list of knowledge triples as strings.\n",
    "\n",
    "Outputs: a structured, human-readable concise version of optimized triples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeOptimizer:\n",
    "    def __init__(self):\n",
    "        self.grouped_info = {}  # Use a regular dictionary\n",
    "\n",
    "    def add_triples(self, triples):\n",
    "        for triple in triples:\n",
    "            try:\n",
    "                subject, predicate, obj = triple.split(\" - \")\n",
    "                \n",
    "                if subject not in self.grouped_info:\n",
    "                    self.grouped_info[subject] = {}\n",
    "                            \n",
    "                if predicate not in self.grouped_info[subject]:\n",
    "                    self.grouped_info[subject][predicate] = set()\n",
    "                \n",
    "                self.grouped_info[subject][predicate].add(obj)\n",
    "            except ValueError:\n",
    "               \n",
    "                print(f\"Skipping malformed triple: {triple}\")\n",
    "\n",
    "    def get_optimized_triples(self):\n",
    "        optimized_list = []\n",
    "        for subject, predicates in self.grouped_info.items():\n",
    "            # Format each predicate-object pair on a new line\n",
    "            predicate_object_pairs = [f\" {pred} - {', '.join(objs)}\" for pred, objs in predicates.items()]\n",
    "            # Join all predicate-object pairs for the subject with newlines\n",
    "            merged_facts = \",\\n\".join(predicate_object_pairs)\n",
    "            optimized_list.append(f\"{subject}: \\n{merged_facts}\")\n",
    "        \n",
    "        optimized_knowledge = \"\\n\".join(optimized_list)\n",
    "        return optimized_knowledge\n",
    "\n",
    "    def reset(self):\n",
    "        self.grouped_info.clear()\n",
    "        \n",
    "\n",
    "# # Example usage\n",
    "# optimizer = KnowledgeOptimizer()\n",
    "# optimizer.add_triples(extracted_knowledge)\n",
    "# optimized_knowledge = optimizer.get_optimized_triples()\n",
    "# print(optimized_knowledge)\n",
    "# optimizer.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PromptCreator: generates a structured prompt by merging input text with optimized knowledge, ensuring external information is clearly integrated.\n",
    "\n",
    "Inputs: raw text (or file path) and structured knowledge.\n",
    "\n",
    "Outputs: a formatted prompt combining both elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptCreator:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def create_prompt(self, input_text, optimized_knowledge):\n",
    "        if os.path.isfile(input_text):\n",
    "            with open(input_text, \"r\", encoding=\"utf-8\") as file:\n",
    "                text = file.read()\n",
    "        else:\n",
    "            text = input_text\n",
    "\n",
    "        augmented_text = f\"\"\"\n",
    "\n",
    "[KNOWLEDGE]\n",
    "{optimized_knowledge}\n",
    "[/KNOWLEDGE]\n",
    "\n",
    "[TEXT]\n",
    "{text}\n",
    "[/TEXT]\n",
    "        \"\"\"\n",
    "           \n",
    "        return augmented_text\n",
    "    \n",
    "#Example usage:\n",
    "# prompt_creator = PromptCreator()\n",
    "# augmented_text = prompt_creator.create_prompt(\"InputText.txt\", optimized_knowledge)\n",
    "# # prompt_creator.save_prompt(augmented_text, \"AugmentedPrompt.txt\")\n",
    "# print(augmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MistralSummarizer: generates summaries using Mistral Large with optional knowledge augmentation to integrate relevant external facts.\n",
    "\n",
    "Inputs:\n",
    "augmented text (includes [KNOWLEDGE] and [TEXT]).\n",
    "\n",
    "compression ratio for output length.\n",
    "\n",
    "knowledge augmentation flag (True/False).\n",
    "\n",
    "Outputs: a concise summary, optionally enriched with bold-highlighted knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralSummarizer:\n",
    "    def __init__(self, api_key):\n",
    "        self.api_key = api_key\n",
    "        self.api_url = \"https://api.mistral.ai/v1/chat/completions\"\n",
    "\n",
    "    def count_tokens(self, augmented_text):\n",
    "        if not hasattr(self, 'tokenizer'):   \n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\n",
    "\n",
    "        tokens = self.tokenizer(augmented_text, return_tensors=\"pt\")\n",
    "        self.n_tokens = len(tokens.input_ids[0])\n",
    "        return self.n_tokens\n",
    "\n",
    "    def summarize(self, augmented_text,  compression_ratio=0.4, knwoledge_augmentation = True):\n",
    "        input_length = self.count_tokens(augmented_text)\n",
    "        num_tokens = max(100, int(input_length * compression_ratio))\n",
    "\n",
    "        if knwoledge_augmentation:\n",
    "            payload = {\n",
    "                \"model\": \"mistral-large-2407\",\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarizes text directly introducing relevant facts from the [KNOWLEDGE] section accurately using **bold**.\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"\"\"Please summarize the following text concisely. When information in the [TEXT] relates to entities mentioned in the [KNOWLEDGE] section, incorporate those relevant facts, using **bold**, to provide additional information.\n",
    "\n",
    "                    {augmented_text}\n",
    "                    \n",
    "                    \"\"\"}\n",
    "                ],\n",
    "                \"max_tokens\": num_tokens,\n",
    "                \"temperature\": 0.4  \n",
    "            }\n",
    "        else:\n",
    "            payload = {\n",
    "                \"model\": \"mistral-large-2407\",\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarizes text directly.\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"\"\"Please summarize the following text concisely. \n",
    "\n",
    "                    {augmented_text}\n",
    "                    \n",
    "                    \"\"\"}\n",
    "                ],\n",
    "                \"max_tokens\": num_tokens,\n",
    "                \"temperature\": 0.4  \n",
    "            }\n",
    "       \n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "\n",
    "        response = requests.post(self.api_url, headers=headers, data=json.dumps(payload))\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Mistral API request failed with status code {response.status_code}: {response.text}\")\n",
    "\n",
    "        result = response.json()\n",
    "        summary = result[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        return summary\n",
    "\n",
    "\n",
    "#  # Example Usage\n",
    "# api_key = \"wOguKQrDCdCAXg9ceIgvlS0a22t9gxEz\"\n",
    "# summarizer = MistralSummarizer(api_key) \n",
    "# summary = summarizer.summarize(augmented_text)\n",
    "# print(f\"\\n Summary: {summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KnowledgeAugmentationPipeline: executes a full knowledge-augmented summarization pipeline, integrating Wikidata knowledge into summaries using Mistral Large.\n",
    "\n",
    "Inputs: path to input text or raw text.\n",
    "\n",
    "Outputs:\n",
    "augmented summary (with knowledge integration).\n",
    "\n",
    "non-augmented summary (standard summarization).\n",
    "\n",
    "structured input prompt used for summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeAugmentationPipeline:\n",
    "    def __init__(self, api_key, use_cache=True):\n",
    "        self.api_key = api_key\n",
    "        self.entity_finder = EntityFinder()\n",
    "        self.cache_manager = CacheManager(use_cache=use_cache)  # Singleton handles caching\n",
    "        self.entity_linker = EntityLinker()\n",
    "        self.knowledge_extractor = KnowledgeExtractor()\n",
    "        self.optimizer = KnowledgeOptimizer()\n",
    "        self.prompt_creator = PromptCreator()\n",
    "        self.summarizer = MistralSummarizer(api_key)\n",
    "    \n",
    "    def process_text(self, input_path):\n",
    "      \n",
    "        extracted_entities = self.entity_finder.ner(input_path)\n",
    "        sorted_entities = self.entity_finder.get_sorted_entities(extracted_entities)\n",
    "\n",
    "        wikidata_entities = self.entity_linker.link_entities(sorted_entities)\n",
    "\n",
    "        extracted_info = self.knowledge_extractor.extract_knowledge(wikidata_entities)\n",
    "\n",
    "        self.optimizer.add_triples(extracted_info)\n",
    "        optimized_knowledge = self.optimizer.get_optimized_triples()\n",
    "\n",
    "        ka_text = self.prompt_creator.create_prompt(input_path, optimized_knowledge)\n",
    "\n",
    "        augmented_summary = self.summarizer.summarize(ka_text, knwoledge_augmentation=True)\n",
    "        non_augmented_summary = self.summarizer.summarize(input_path, knwoledge_augmentation=False)\n",
    "       \n",
    "        self.optimizer.reset()\n",
    "       \n",
    "        return augmented_summary, non_augmented_summary, ka_text\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_article = \"InputText\"  \n",
    "\n",
    "pipeline = KnowledgeAugmentationPipeline(api_key=\"wOguKQrDCdCAXg9ceIgvlS0a22t9gxEz\", use_cache=False)\n",
    "with open(\"comparisons.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "     \n",
    "        augmented_summary, non_augmented_summary, ka_text = pipeline.process_text(full_article)\n",
    "\n",
    "        f.write(f\"Article:\\n\")\n",
    "        f.write(f\"Knowledge-Augmented Text (ka_text):\\n{ka_text}\\n\\n\")\n",
    "        f.write(f\"Augmented Summary:\\n{augmented_summary}\\n\\n\")\n",
    "        f.write(f\"Non-Augmented Summary:\\n{non_augmented_summary}\\n\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\\n\") \n",
    "\n",
    "print(\"Summarization results saved to comparisons.txt.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize your Knowledge Augmentation Pipeline\n",
    "pipeline = KnowledgeAugmentationPipeline(api_key=\"wOguKQrDCdCAXg9ceIgvlS0a22t9gxEz\", use_cache=True)\n",
    "  # Number of articles to process\n",
    "\n",
    "# Load the XSum dataset \n",
    "num_articles = 1\n",
    "dataset = load_dataset(\"xsum\", split=\"test\", trust_remote_code=True)  \n",
    "\n",
    "\n",
    "# Open file to save results\n",
    "with open(\"comparisons.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "    for i, article in enumerate(dataset):\n",
    "        if i >= num_articles:  # Stop after processing the specified number of articles\n",
    "            break\n",
    "\n",
    "        full_article = article[\"document\"]  # Access the full article text\n",
    "         # Access the reference summary (if needed)\n",
    "        # Process the article to get both augmented and non-augmented summaries\n",
    "        augmented_summary, non_augmented_summary, ka_text = pipeline.process_text(full_article)\n",
    "\n",
    "        # Save results to the file\n",
    "        f.write(f\"Article {i+1}:\\n\")\n",
    "        f.write(f\"Knowledge-Augmented Text (ka_text):\\n{ka_text}\\n\\n\")\n",
    "        f.write(f\"Augmented Summary:\\n{augmented_summary}\\n\\n\")\n",
    "        f.write(f\"Non-Augmented Summary:\\n{non_augmented_summary}\\n\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\\n\")  # Separator for readability\n",
    "\n",
    "print(\"Summarization results saved to sum_results.txt.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
